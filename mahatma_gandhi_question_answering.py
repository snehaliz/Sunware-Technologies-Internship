# -*- coding: utf-8 -*-
"""Mahatma Gandhi Question Answering.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BoLVnA_npM887WsiIFF5AmUCvYeZlsXB
"""

#https://medium.com/@patonw/question-answering-with-pytorch-transformers-part-3-d67ac06a23b7
!pip install transformers

import torch

#This class BertForQuestionAnswering provides fine tuning and this model has specifically been fine tuned
#for the squad benchmark. There are lots of pre-trained models that can be referenced from the transformers library. 
#Masked language modeling is predicting what word will fill in the blank of a sentence. 
#Whole word masking means that all tokens that are words can be masked at the same time. 
from transformers import BertForQuestionAnswering
model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad', return_dict = False)

#BertTokenizer either splits a word such that 1 token is 1 word or 1 word is split into multiple parts to create multiple tokens. 
#https://huggingface.co/bert-large-uncased-whole-word-masking-finetuned-squad
from transformers import BertTokenizer

tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')

#This part of the code takes in question and answer prompts for the model to then find the answer for the question based on the answer_text. 
#question = "How many parameters does BERT-large have?"
#question = "How many nations does this region include territory?"
#question = "Where do I live?"
#question = "What was Mahatma Gandhi's full name?" #Answer included Mohandas Karamchand Gandi if put full name but only Mohandas Karamchand if actual name
#question = "What did Mahatma Gandhi do?" #Answer: lead ... british rule
#question = "What were Mahatma Gandhi's jobs?" #Answer: lawyer for "job" but all three show up when asking "jobs"
#question = "How did Mahatma Gandhi impact others?" #Answer: "inspire movements for civil rights and freedom across the world"
question = "What was Mahatma Gandhi's strategy in his leadership?" #Answer: nonviolent resistance
#answer_text = "BERT-large is really big... it has 24-layers and an embedding size of 1,024, for a total of 340M parameters! Altogether it is 1.34GB, so expect it to take a couple minutes to download to your Colab instance."
#answer_text = "The Amazon rainforest (Portuguese: Floresta Amazônica or Amazônia; Spanish: Selva Amazónica, Amazonía or usually Amazonia; French: Forêt amazonienne; Dutch: Amazoneregenwoud), also known in English as Amazonia or the Amazon Jungle, is a moist broadleaf forest that covers most of the Amazon basin of South America; This basin encompasses 7,000,000 square kilometres (2,700,000 sq mi), of which 5,500,000 square kilometres (2,100,000 sq mi) are covered by the rainforest; This region includes territory belonging to nine nations. The majority of the forest is contained within Brazil, with 60% of the rainforest, followed by Peru with 13%, Colombia with 10%, and with minor amounts in Venezuela, Ecuador, Bolivia, Guyana, Suriname and French Guiana; States or departments in four nations contain 'Amazonas' in their names; The Amazon represents over half of the planet's remaining rainforests, and comprises the largest and most biodiverse tract of tropical rainforest in the world, with an estimated 390 billion individual trees divided into 16,000 species."

#answer_text = "My name is Sarah and I live in London"
answer_text = "Mohandas Karamchand Gandhi was an Indian lawyer, anti-colonial nationalist and political ethicist who employed nonviolent resistance to lead the successful campaign for India's independence from British rule, and to later inspire movements for civil rights and freedom across the world."

# Apply the tokenizer to the input text, treating them as a text-pair. Tokenizer.encode returns the strings
#to a sequence of ids from the vocabulary and the tokenizer. The question and answer_text are concatenated together here with a SEP token in 
#between them.  
#https://huggingface.co/docs/transformers/main_classes/tokenizer
input_ids = tokenizer.encode(question, answer_text)

print('The input has a total of {:} tokens.'.format(len(input_ids)))

# BERT only needs the token IDs, but for the purpose of inspecting the 
# tokenizer's behavior, let's also get the token strings and display them.

#tokenizer.convert converts the sequence of integers to tokens. 
#The for loop then goes through each token and the id of the token and prints a space if the id is of a sep_token
#which is a token that separates 2 different sentences in the same input. Otherwise, the token and id are printed out in a certain format. 
tokens = tokenizer.convert_ids_to_tokens(input_ids)

# For each token and its id...
for token, id in zip(tokens, input_ids):
    
    # If this is the [SEP] token, add some space around it to make it stand out.
    if id == tokenizer.sep_token_id:
        print('')
    
    # Print the token string and its ID in two columns.
    print('{:<12} {:>6,}'.format(token, id))

    if id == tokenizer.sep_token_id:
        print('')

#BERT needs to be able to distinguish between the question and answer text. BERT has token, segment, and position embeddings and 
#for segment embeddings, BERT has two which are segments A and B. First, segment A embeddings need to be added to the question and segment B 
#embeddings need to be added to the answer text before token embeddings can be added. Embeddings are numerical representations of objects and
#relationships.  

# Search the input_ids for the first instance of the `[SEP]` token.
sep_index = input_ids.index(tokenizer.sep_token_id)

# The number of segment A tokens includes the [SEP] token istelf.
num_seg_a = sep_index + 1
#print("Number of segment A tokens:", num_seg_a)

# The remainder are segment B.
num_seg_b = len(input_ids) - num_seg_a
#print("Number of segment B tokens:", num_seg_b)

# Construct the list of 0s and 1s.
segment_ids = [0]*num_seg_a + [1]*num_seg_b
#print("Segment ids:", segment_ids)

# There should be a segment_id for every input token.
assert len(segment_ids) == len(input_ids)

# Run our example through the model. This code creates 2 arrays that are the same length as the input_ids. The start_scores
#contains activation values telling the likelihood that the token is the start of an answer and the end_scores tell activation values
#of the likelihood that the token is the end of the answer. 
start_scores, end_scores = model(torch.tensor([input_ids]), # The tokens representing our input text.
                                 token_type_ids=torch.tensor([segment_ids])) # The segment IDs to differentiate question from answer_text
#print("Input ids:", input_ids)
#print("Segment ids:", segment_ids)
#print("Start scores:", start_scores)
#print("End scores:", end_scores)

# Find the tokens with the highest `start` and `end` scores. https://mccormickml.com/2020/03/10/question-answering-with-a-fine-tuned-BERT/#1-install-huggingface-transformers-library
#We then obtain the answer by slicing between strongest activations in start scores and end scores. 
answer_start = torch.argmax(start_scores)
#print("Answer start:", answer_start)
answer_end = torch.argmax(end_scores)
#print("Answer end:", answer_end)

# Combine the tokens in the answer and print it out.
answer = ' '.join(tokens[answer_start:answer_end+1])

print('Answer: "' + answer + '"')

